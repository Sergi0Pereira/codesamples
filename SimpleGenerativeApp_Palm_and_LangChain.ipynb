{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Generative App Using Palm 2 & LangChain\n",
        "\n",
        "In this simple app, we will illustrate few ways of invoking Vertex AI:\n",
        "- One using the Google API Key (obtainable through markersuite.google.com\n",
        "- One using Google Cloud after authenticating through google_auth or using service account"
      ],
      "metadata": {
        "id": "Uj7R48-ZXmTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/guruvittal/codesamples/blob/main/SimpleGenerativeApp_Palm_and_LangChain.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/guruvittal/codesamples/blob/main/SimpleGenerativeApp_Palm_and_LangChain.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/guruvittal/codesamples/main/SimpleGenerativeApp_Palm_and_LangChain.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "BgK3Mb8pcv8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation & Authentication\n",
        "\n",
        "**Install google-generativeai & langchain**"
      ],
      "metadata": {
        "id": "SPG8eRQCcNQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install langchain & Google Generative AI SDK (Usable with Makersuite end point)\n",
        "!pip install langchain google-generativeai\n",
        "\n",
        "# Install Vertex AI LLM SDK (Google Cloud Vertex AI endpoint)\n",
        "! pip install google-cloud-aiplatform==1.25.0\n",
        "\n"
      ],
      "metadata": {
        "id": "IdsE3DEJcM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authenticate & Initialize Google Cloud Project & API Key**"
      ],
      "metadata": {
        "id": "ptwMB9pqcniz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Authentication & Initialize access through Google Cloud\n",
        "import sys\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "#Vertex in GCP End point\n",
        "PROJECT_ID = \"PROJECT_ID\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "#Access the Palm API using Google API Key\n",
        "#MakerSuite End point\n",
        "GOOGLE_API_KEY='API_KEY' # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "HP80SWi0rIBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Vertex AI Base Model & Class**\n",
        "\n",
        "***In this section, we will define base classes and libraries for use  "
      ],
      "metadata": {
        "id": "4lcArn48r6pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "\n",
        "from pydantic import BaseModel, root_validator\n",
        "from typing import Any, Mapping, Optional, List, Dict\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "class _VertexCommon(BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    client: Any = None #: :meta private:\n",
        "    model_name: str = \"text-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    temperature: float = 0.2\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    top_p: int = 0.8\n",
        "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
        "\n",
        "    top_k: int = 40\n",
        "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
        "\n",
        "    max_output_tokens: int = 200\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _default_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
        "        return {\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"max_output_tokens\": self.max_output_tokens\n",
        "        }\n",
        "\n",
        "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
        "        res = self.client.predict(prompt, **self._default_params)\n",
        "        return self._enforce_stop_words(res.text, stop)\n",
        "\n",
        "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if stop:\n",
        "            return enforce_stop_tokens(text, stop)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"vertex_ai\"\n",
        "\n",
        "class VertexLLM(_VertexCommon, LLM):\n",
        "    model_name: str = \"text-bison@001\"\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextGenerationModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call out to Vertex AI's create endpoint.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to pass into the model.\n",
        "\n",
        "        Returns:\n",
        "            The string generated by the model.\n",
        "        \"\"\"\n",
        "        return self._predict(prompt, stop)\n",
        "\n"
      ],
      "metadata": {
        "id": "ax6hlCt7YbXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078ea183-31a8-4d27-aa70-2e456b3361e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiatlize Vertex AI"
      ],
      "metadata": {
        "id": "7yHf4ipxYyfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vertex AI SDK\n",
        "import vertexai\n",
        "PROJECT_ID = \"PROJECT_ID\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "EZe8iS2CY2E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoking Text Bison directly"
      ],
      "metadata": {
        "id": "-EeiGb_cnThu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = VertexLLM(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "#Asking Text Bison the question directly\n",
        "print(\"Response for direct question: WHo was the president of US when Justin Bieber was born\")\n",
        "response = llm('WHo was the president of US when Justin Bieber was born')\n",
        "print(response)\n",
        "\n",
        "print(\"Response for prompt engineered question: WHo was the president of US when Justin Bieber was born\")\n",
        "#Asking Text Bison with some prompt engineering\n",
        "response = llm('Think through step by step: WHo was the president of US when Justin Bieber was born')\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hloE0JOahc3Q",
        "outputId": "7ede83cf-0d6a-4dc9-def8-de887e417b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for direct question: WHo was the president of US when Justin Bieber was born\n",
            " George Walker Bush\n",
            "Response for prompt engineered question: WHo was the president of US when Justin Bieber was born\n",
            " Justin Bieber was born on March 1, 1994. Bill Clinton was the president of the United States from January 20, 1993, to January 20, 2001. Therefore, Bill Clinton was the president of the United States when Justin Bieber was born.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Invoking Chat Bison - Raw"
      ],
      "metadata": {
        "id": "PmE0xEvkl9MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Invoke Chat Bison*"
      ],
      "metadata": {
        "id": "gTkOBnHyfxVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import google.generativeai as palm\n",
        "\n",
        "palm.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "defaults = {\n",
        "  'model': 'models/chat-bison-001',\n",
        "  'temperature': 0.25,\n",
        "  'candidate_count': 1,\n",
        "  'top_k': 40,\n",
        "  'top_p': 0.95,\n",
        "}\n",
        "context = \"You are a historian and sports buff.\"\n",
        "examples = [\n",
        "  [\n",
        "    \"Who is the President of USA\",\n",
        "    \"Joe Biden\"\n",
        "  ],\n",
        "  [\n",
        "    \"Who was the President of USA in 2020\",\n",
        "    \"Donald Trump\"\n",
        "  ]\n",
        "]\n",
        "messages = [\n",
        "  \"Where do US Presidents live\",\n",
        "  \"White House, Washington DC\"\n",
        "]\n",
        "messages.append(\"Who was the President of US when Justin Bieber was born?\")\n",
        "response = palm.chat(\n",
        "  **defaults,\n",
        "  context=context,\n",
        "  examples=examples,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(\"context:\", context)\n",
        "print(\"examples:\", examples)\n",
        "\n",
        "print(\"messages:\", messages)\n",
        "\n",
        "print(response.last) # Response of the AI to your most recent request"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkKnALpQzQm2",
        "outputId": "58a23a74-407c-4897-a4b8-bd688a1acf5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: You are a historian and sports buff.\n",
            "examples: [['Who is the President of USA', 'Joe Biden'], ['Who was the President of USA in 2020', 'Donald Trump']]\n",
            "messages: ['Where do US Presidents live', 'White House, Washington DC', 'Who was the President of US when Justin Bieber was born?']\n",
            "Justin Bieber was born on March 1, 1994. Bill Clinton was the President of the United States at the time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Invoking Text Bison Using LangChain Prompt Template"
      ],
      "metadata": {
        "id": "19iifnZ5luA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Building a Prompt Template with LangChain*"
      ],
      "metadata": {
        "id": "blSr3Ihfdw7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = VertexLLM(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(llm(\"Hello\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV4BShNNvYgt",
        "outputId": "20c708d1-8947-4b22-ac3d-775d14bedca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello! How may I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import VertexAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "question = \"Who was the president in the year Justin Beiber was born?\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmPJuLcsq9qV",
        "outputId": "ccbe83b4-e0ee-434b-d1fc-4a9b43235831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Justin Bieber was born on March 1, 1994. Bill Clinton was the president of the United States from January 20, 1993, to January 20, 2001.\n",
            "The final answer is Bill Clinton\n"
          ]
        }
      ]
    }
  ]
}
