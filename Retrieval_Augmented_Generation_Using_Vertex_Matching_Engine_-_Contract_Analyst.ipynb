{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj7R48-ZXmTC"
      },
      "source": [
        "# Retrieval Augmented Generation - Procurement Contract Analyst -  Palm2 & LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/guruvittal/codesamples/blob/main/Retrieval_Augmented_Generation_Using_Vertex_Matching_Engine_-_Contract_Analyst.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/guruvittal/codesamples/blob/main/Retrieval_Augmented_Generation_Using_Vertex_Matching_Engine_-_Contract_Analyst.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/guruvittal/codesamples/main/Retrieval_Augmented_Generation_Using_Vertex_Matching_Engine_-_Contract_Analyst.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "2D5-yDE6Z7NX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPG8eRQCcNQ3"
      },
      "source": [
        "## Installation & Authentication\n",
        "\n",
        "**Install google-cloud-aiplatform & langchain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdsE3DEJcM35"
      },
      "outputs": [],
      "source": [
        "# Install langchain and related libraries\n",
        "!pip install langchain unstructured\n",
        "\n",
        "# Install Vertex AI LLM SDK\n",
        "! pip install google-cloud-aiplatform\n",
        "\n",
        "# Gradio as Frontend\n",
        "!pip install -q gradio\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwMB9pqcniz"
      },
      "source": [
        "**Authenticate**\n",
        "\n",
        "Within colab, a simple user authentication is adequate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP80SWi0rIBL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get Libraries & Classes"
      ],
      "metadata": {
        "id": "oEX1X9z0VF1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LangChain Libraries"
      ],
      "metadata": {
        "id": "uV6PX1_hT5Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.vectorstores import MatchingEngine\n",
        "\n",
        "\"\"\"Vertex Matching Engine implementation of the vector store.\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "import uuid\n",
        "from typing import Any, Iterable, List, Optional, Type\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import TensorflowHubEmbeddings\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import json\n"
      ],
      "metadata": {
        "id": "8smAwjNcT9hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lcArn48r6pZ"
      },
      "source": [
        "### Vertex Libraries, Classes & Helper Functions\n",
        "**Reference Libraries**\n",
        "\n",
        "In this section, we will identify all the library classes that will be referenced in the code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax6hlCt7YbXx",
        "outputId": "7c799183-35b4-4a0e-8d68-8085fb5a0aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.35.0\n"
          ]
        }
      ],
      "source": [
        "# Using Vertex AI\n",
        "import vertexai\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "# Using Google Cloud Storage Directory loader from langchain\n",
        "from langchain.document_loaders import GCSDirectoryLoader\n",
        "\n",
        "import time\n",
        "\n",
        "from pydantic import BaseModel, Extra, root_validator\n",
        "from typing import Any, Mapping, Optional, List, Dict\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "# Create chain to answer questions\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Google Cloud Libraries\n",
        "from google.cloud import storage\n",
        "from google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint\n",
        "from google.cloud import aiplatform_v1\n",
        "from google.oauth2.service_account import Credentials\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "# Additional needed libraries\n",
        "import gradio as gr\n",
        "import markdown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yHf4ipxYyfy"
      },
      "source": [
        "## Initiatlize Vertex AI\n",
        "\n",
        "**We will need a project id and location where the Vertex compute and embedding will be hosted**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZe8iS2CY2E8"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"argolis-project-340214\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0htdnYAHonv"
      },
      "source": [
        "## Build the Matching Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwJlBNwIy0y"
      },
      "source": [
        "\n",
        "\n",
        "###*Define Parameters & Create Bucket for Embeddings*\n",
        "Set the locations of the documents, embeddings, index and dimensions for the embedding vector*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPvIUvqJg_fi",
        "outputId": "2882e51c-5580-4896-e003-92fcc7f6106d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ gsutil mb -p argolis-project-340214 -l us-central1 gs://argolis-project-340214-me-bucket\n",
            "Creating gs://argolis-project-340214-me-bucket/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'argolis-project-340214-me-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ],
      "source": [
        "ME_BUCKET = \"matching_engine_bucket\"\n",
        "ME_REGION = \"us-central1\"\n",
        "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"  # @param {type:\"string\"}\n",
        "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"  # @param {type:\"string\"}\n",
        "ME_DIMENSIONS = 768  # @param {type:\"integer\"} when using Vertex PaLM Embedding\n",
        "\n",
        "! set -x && gsutil mb -p $PROJECT_ID -l us-central1 gs://$ME_EMBEDDING_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Define Matching Engine Class & Utilities Class*"
      ],
      "metadata": {
        "id": "E4AXBRDVIADR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY9n_20-i0oO"
      },
      "outputs": [],
      "source": [
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "class MatchingEngine(VectorStore):\n",
        "    \"\"\"Vertex Matching Engine implementation of the vector store.\n",
        "\n",
        "    While the embeddings are stored in the Matching Engine, the embedded\n",
        "    documents will be stored in GCS.\n",
        "\n",
        "    An existing Index and corresponding Endpoint are preconditions for\n",
        "    using this module.\n",
        "\n",
        "    See usage in docs/modules/indexes/vectorstores/examples/matchingengine.ipynb\n",
        "\n",
        "    Note that this implementation is mostly meant for reading if you are\n",
        "    planning to do a real time implementation. While reading is a real time\n",
        "    operation, updating the index takes close to one hour.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        index: MatchingEngineIndex,\n",
        "        endpoint: MatchingEngineIndexEndpoint,\n",
        "        embedding: Embeddings,\n",
        "        gcs_client: storage.Client,\n",
        "        index_client: aiplatform_v1.IndexServiceClient,\n",
        "        index_endpoint_client: aiplatform_v1.IndexEndpointServiceClient,\n",
        "        gcs_bucket_name: str,\n",
        "        credentials: Credentials = None,\n",
        "    ):\n",
        "        \"\"\"Vertex Matching Engine implementation of the vector store.\n",
        "\n",
        "        While the embeddings are stored in the Matching Engine, the embedded\n",
        "        documents will be stored in GCS.\n",
        "\n",
        "        An existing Index and corresponding Endpoint are preconditions for\n",
        "        using this module.\n",
        "\n",
        "        See usage in\n",
        "        docs/modules/indexes/vectorstores/examples/matchingengine.ipynb.\n",
        "\n",
        "        Note that this implementation is mostly meant for reading if you are\n",
        "        planning to do a real time implementation. While reading is a real time\n",
        "        operation, updating the index takes close to one hour.\n",
        "\n",
        "        Attributes:\n",
        "            project_id: The GCS project id.\n",
        "            index: The created index class. See\n",
        "            ~:func:`MatchingEngine.from_components`.\n",
        "            endpoint: The created endpoint class. See\n",
        "            ~:func:`MatchingEngine.from_components`.\n",
        "            embedding: A :class:`Embeddings` that will be used for\n",
        "            embedding the text sent. If none is sent, then the\n",
        "            multilingual Tensorflow Universal Sentence Encoder will be used.\n",
        "            gcs_client: The Google Cloud Storage client.\n",
        "            credentials (Optional): Created GCP credentials.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._validate_google_libraries_installation()\n",
        "\n",
        "        self.project_id = project_id\n",
        "        self.region = region\n",
        "        self.index = index\n",
        "        self.endpoint = endpoint\n",
        "        self.embedding = embedding\n",
        "        self.gcs_client = gcs_client\n",
        "        self.index_client = index_client\n",
        "        self.index_endpoint_client = index_endpoint_client\n",
        "        self.gcs_client = gcs_client\n",
        "        self.credentials = credentials\n",
        "        self.gcs_bucket_name = gcs_bucket_name\n",
        "\n",
        "    def _validate_google_libraries_installation(self) -> None:\n",
        "        \"\"\"Validates that Google libraries that are needed are installed.\"\"\"\n",
        "        try:\n",
        "            from google.cloud import aiplatform, storage  # noqa: F401\n",
        "            from google.oauth2 import service_account  # noqa: F401\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                \"You must run `pip install --upgrade \"\n",
        "                \"google-cloud-aiplatform google-cloud-storage`\"\n",
        "                \"to use the MatchingEngine Vectorstore.\"\n",
        "            )\n",
        "\n",
        "    def add_texts(\n",
        "        self,\n",
        "        texts: Iterable[str],\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
        "\n",
        "        Args:\n",
        "            texts: Iterable of strings to add to the vectorstore.\n",
        "            metadatas: Optional list of metadatas associated with the texts.\n",
        "            kwargs: vectorstore specific parameters.\n",
        "\n",
        "        Returns:\n",
        "            List of ids from adding the texts into the vectorstore.\n",
        "        \"\"\"\n",
        "        logger.debug(\"Embedding documents.\")\n",
        "        embeddings = self.embedding.embed_documents(list(texts))\n",
        "        insert_datapoints_payload = []\n",
        "        ids = []\n",
        "\n",
        "        # Streaming index update\n",
        "        for idx, (embedding, text, metadata) in enumerate(\n",
        "            zip(embeddings, texts, metadatas)\n",
        "        ):\n",
        "            id = uuid.uuid4()\n",
        "            ids.append(id)\n",
        "            self._upload_to_gcs(text, f\"documents/{id}\")\n",
        "            metadatas[idx]\n",
        "            insert_datapoints_payload.append(\n",
        "                aiplatform_v1.IndexDatapoint(\n",
        "                    datapoint_id=str(id),\n",
        "                    feature_vector=embedding,\n",
        "                    restricts=metadata if metadata else [],\n",
        "                )\n",
        "            )\n",
        "            if idx % 100 == 0:\n",
        "                upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
        "                    index=self.index.name, datapoints=insert_datapoints_payload\n",
        "                )\n",
        "                response = self.index_client.upsert_datapoints(request=upsert_request)\n",
        "                insert_datapoints_payload = []\n",
        "        if len(insert_datapoints_payload) > 0:\n",
        "            upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
        "                index=self.index.name, datapoints=insert_datapoints_payload\n",
        "            )\n",
        "            _ = self.index_client.upsert_datapoints(request=upsert_request)\n",
        "\n",
        "        logger.debug(\"Updated index with new configuration.\")\n",
        "        logger.info(f\"Indexed {len(ids)} documents to Matching Engine.\")\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def _upload_to_gcs(self, data: str, gcs_location: str) -> None:\n",
        "        \"\"\"Uploads data to gcs_location.\n",
        "\n",
        "        Args:\n",
        "            data: The data that will be stored.\n",
        "            gcs_location: The location where the data will be stored.\n",
        "        \"\"\"\n",
        "        bucket = self.gcs_client.get_bucket(self.gcs_bucket_name)\n",
        "        blob = bucket.blob(gcs_location)\n",
        "        blob.upload_from_string(data)\n",
        "\n",
        "    def get_matches(\n",
        "        self,\n",
        "        embeddings: List[str],\n",
        "        n_matches: int,\n",
        "        index_endpoint: MatchingEngineIndexEndpoint,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        get matches from matching engine given a vector query\n",
        "        Uses public endpoint\n",
        "\n",
        "        \"\"\"\n",
        "        import requests\n",
        "        import json\n",
        "\n",
        "        request_data = {\n",
        "            \"deployed_index_id\": index_endpoint.deployed_indexes[0].id,\n",
        "            \"return_full_datapoint\": True,\n",
        "            \"queries\": [\n",
        "                {\n",
        "                    \"datapoint\": {\"datapoint_id\": f\"{i}\", \"feature_vector\": emb},\n",
        "                    \"neighbor_count\": n_matches,\n",
        "                }\n",
        "                for i, emb in enumerate(embeddings)\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        endpoint_address = self.endpoint.public_endpoint_domain_name\n",
        "        rpc_address = f\"https://{endpoint_address}/v1beta1/{index_endpoint.resource_name}:findNeighbors\"\n",
        "        endpoint_json_data = json.dumps(request_data)\n",
        "\n",
        "        logger.debug(f\"Querying Matching Engine Index Endpoint {rpc_address}\")\n",
        "\n",
        "        request = google.auth.transport.requests.Request()\n",
        "        self.credentials.refresh(request)\n",
        "        header = {\"Authorization\": \"Bearer \" + self.credentials.token}\n",
        "\n",
        "        return requests.post(rpc_address, data=endpoint_json_data, headers=header)\n",
        "\n",
        "    def similarity_search(\n",
        "        self, query: str, k: int = 4, search_distance: float = 0.65, **kwargs: Any\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"Return docs most similar to query.\n",
        "\n",
        "        Args:\n",
        "            query: The string that will be used to search for similar documents.\n",
        "            k: The amount of neighbors that will be retrieved.\n",
        "            search_distance: filter search results by  search distance by adding a threshold value\n",
        "\n",
        "        Returns:\n",
        "            A list of k matching documents.\n",
        "        \"\"\"\n",
        "\n",
        "        logger.debug(f\"Embedding query {query}.\")\n",
        "        embedding_query = self.embedding.embed_documents([query])\n",
        "        deployed_index_id = self._get_index_id()\n",
        "        logger.debug(f\"Deployed Index ID = {deployed_index_id}\")\n",
        "\n",
        "        # TO-DO: Pending query sdk integration\n",
        "        # response = self.endpoint.match(\n",
        "        #     deployed_index_id=self._get_index_id(),\n",
        "        #     queries=embedding_query,\n",
        "        #     num_neighbors=k,\n",
        "        # )\n",
        "\n",
        "        response = self.get_matches(embedding_query, k, self.endpoint)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            response = response.json()[\"nearestNeighbors\"]\n",
        "        else:\n",
        "            raise Exception(f\"Failed to query index {str(response)}\")\n",
        "\n",
        "        if len(response) == 0:\n",
        "            return []\n",
        "\n",
        "        logger.debug(f\"Found {len(response)} matches for the query {query}.\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # I'm only getting the first one because queries receives an array\n",
        "        # and the similarity_search method only recevies one query. This\n",
        "        # means that the match method will always return an array with only\n",
        "        # one element.\n",
        "        for doc in response[0][\"neighbors\"]:\n",
        "            page_content = self._download_from_gcs(\n",
        "                f\"documents/{doc['datapoint']['datapointId']}\"\n",
        "            )\n",
        "            metadata = {}\n",
        "            if \"restricts\" in doc[\"datapoint\"]:\n",
        "                metadata = {\n",
        "                    item[\"namespace\"]: item[\"allowList\"][0]\n",
        "                    for item in doc[\"datapoint\"][\"restricts\"]\n",
        "                }\n",
        "            if \"distance\" in doc:\n",
        "                metadata[\"score\"] = doc[\"distance\"]\n",
        "                if doc[\"distance\"] >= search_distance:\n",
        "                    results.append(\n",
        "                        Document(page_content=page_content, metadata=metadata)\n",
        "                    )\n",
        "            else:\n",
        "                results.append(Document(page_content=page_content, metadata=metadata))\n",
        "\n",
        "        logger.debug(\"Downloaded documents for query.\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _get_index_id(self) -> str:\n",
        "        \"\"\"Gets the correct index id for the endpoint.\n",
        "\n",
        "        Returns:\n",
        "            The index id if found (which should be found) or throws\n",
        "            ValueError otherwise.\n",
        "        \"\"\"\n",
        "        for index in self.endpoint.deployed_indexes:\n",
        "            if index.index == self.index.name:\n",
        "                return index.id\n",
        "\n",
        "        raise ValueError(\n",
        "            f\"No index with id {self.index.name} \"\n",
        "            f\"deployed on enpoint \"\n",
        "            f\"{self.endpoint.display_name}.\"\n",
        "        )\n",
        "\n",
        "    def _download_from_gcs(self, gcs_location: str) -> str:\n",
        "        \"\"\"Downloads from GCS in text format.\n",
        "\n",
        "        Args:\n",
        "            gcs_location: The location where the file is located.\n",
        "\n",
        "        Returns:\n",
        "            The string contents of the file.\n",
        "        \"\"\"\n",
        "        bucket = self.gcs_client.get_bucket(self.gcs_bucket_name)\n",
        "        try:\n",
        "            blob = bucket.blob(gcs_location)\n",
        "            return blob.download_as_string()\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_texts(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        texts: List[str],\n",
        "        embedding: Embeddings,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Use from components instead.\"\"\"\n",
        "        raise NotImplementedError(\n",
        "            \"This method is not implemented. Instead, you should initialize the class\"\n",
        "            \" with `MatchingEngine.from_components(...)` and then call \"\n",
        "            \"`from_texts`\"\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_documents(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        documents: List[str],\n",
        "        embedding: Embeddings,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Use from components instead.\"\"\"\n",
        "        raise NotImplementedError(\n",
        "            \"This method is not implemented. Instead, you should initialize the class\"\n",
        "            \" with `MatchingEngine.from_components(...)` and then call \"\n",
        "            \"`from_documents`\"\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_components(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        gcs_bucket_name: str,\n",
        "        index_id: str,\n",
        "        endpoint_id: str,\n",
        "        credentials_path: Optional[str] = None,\n",
        "        embedding: Optional[Embeddings] = None,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Takes the object creation out of the constructor.\n",
        "\n",
        "        Args:\n",
        "            project_id: The GCP project id.\n",
        "            region: The default location making the API calls. It must have\n",
        "            the same location as the GCS bucket and must be regional.\n",
        "            gcs_bucket_name: The location where the vectors will be stored in\n",
        "            order for the index to be created.\n",
        "            index_id: The id of the created index.\n",
        "            endpoint_id: The id of the created endpoint.\n",
        "            credentials_path: (Optional) The path of the Google credentials on\n",
        "            the local file system.\n",
        "            embedding: The :class:`Embeddings` that will be used for\n",
        "            embedding the texts.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngine with the texts added to the index.\n",
        "        \"\"\"\n",
        "        gcs_bucket_name = cls._validate_gcs_bucket(gcs_bucket_name)\n",
        "\n",
        "        # Set credentials\n",
        "        if credentials_path:\n",
        "            credentials = cls._create_credentials_from_file(credentials_path)\n",
        "        else:\n",
        "            credentials, _ = google.auth.default()\n",
        "            request = google.auth.transport.requests.Request()\n",
        "            credentials.refresh(request)\n",
        "\n",
        "        index = cls._create_index_by_id(index_id, project_id, region, credentials)\n",
        "        endpoint = cls._create_endpoint_by_id(\n",
        "            endpoint_id, project_id, region, credentials\n",
        "        )\n",
        "\n",
        "        gcs_client = cls._get_gcs_client(credentials, project_id)\n",
        "        index_client = cls._get_index_client(project_id, region, credentials)\n",
        "        index_endpoint_client = cls._get_index_endpoint_client(\n",
        "            project_id, region, credentials\n",
        "        )\n",
        "        cls._init_aiplatform(project_id, region, gcs_bucket_name, credentials)\n",
        "\n",
        "        return cls(\n",
        "            project_id=project_id,\n",
        "            region=region,\n",
        "            index=index,\n",
        "            endpoint=endpoint,\n",
        "            embedding=embedding or cls._get_default_embeddings(),\n",
        "            gcs_client=gcs_client,\n",
        "            index_client=index_client,\n",
        "            index_endpoint_client=index_endpoint_client,\n",
        "            credentials=credentials,\n",
        "            gcs_bucket_name=gcs_bucket_name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _validate_gcs_bucket(cls, gcs_bucket_name: str) -> str:\n",
        "        \"\"\"Validates the gcs_bucket_name as a bucket name.\n",
        "\n",
        "        Args:\n",
        "              gcs_bucket_name: The received bucket uri.\n",
        "\n",
        "        Returns:\n",
        "              A valid gcs_bucket_name or throws ValueError if full path is\n",
        "              provided.\n",
        "        \"\"\"\n",
        "        gcs_bucket_name = gcs_bucket_name.replace(\"gs://\", \"\")\n",
        "        if \"/\" in gcs_bucket_name:\n",
        "            raise ValueError(\n",
        "                f\"The argument gcs_bucket_name should only be \"\n",
        "                f\"the bucket name. Received {gcs_bucket_name}\"\n",
        "            )\n",
        "        return gcs_bucket_name\n",
        "\n",
        "    @classmethod\n",
        "    def _create_credentials_from_file(\n",
        "        cls, json_credentials_path: Optional[str]\n",
        "    ) -> Optional[Credentials]:\n",
        "        \"\"\"Creates credentials for GCP.\n",
        "\n",
        "        Args:\n",
        "             json_credentials_path: The path on the file system where the\n",
        "             credentials are stored.\n",
        "\n",
        "         Returns:\n",
        "             An optional of Credentials or None, in which case the default\n",
        "             will be used.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.oauth2 import service_account\n",
        "\n",
        "        credentials = None\n",
        "        if json_credentials_path is not None:\n",
        "            credentials = service_account.Credentials.from_service_account_file(\n",
        "                json_credentials_path\n",
        "            )\n",
        "\n",
        "        return credentials\n",
        "\n",
        "    @classmethod\n",
        "    def _create_index_by_id(\n",
        "        cls, index_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> MatchingEngineIndex:\n",
        "        \"\"\"Creates a MatchingEngineIndex object by id.\n",
        "\n",
        "        Args:\n",
        "            index_id: The created index id.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngineIndex.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        logger.debug(f\"Creating matching engine index with id {index_id}.\")\n",
        "        index_client = cls._get_index_client(project_id, region, credentials)\n",
        "        request = aiplatform_v1.GetIndexRequest(name=index_id)\n",
        "        return index_client.get_index(request=request)\n",
        "\n",
        "    @classmethod\n",
        "    def _create_endpoint_by_id(\n",
        "        cls, endpoint_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> MatchingEngineIndexEndpoint:\n",
        "        \"\"\"Creates a MatchingEngineIndexEndpoint object by id.\n",
        "\n",
        "        Args:\n",
        "            endpoint_id: The created endpoint id.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngineIndexEndpoint.\n",
        "            :param project_id:\n",
        "            :param region:\n",
        "            :param credentials:\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform\n",
        "\n",
        "        logger.debug(f\"Creating endpoint with id {endpoint_id}.\")\n",
        "        return aiplatform.MatchingEngineIndexEndpoint(\n",
        "            index_endpoint_name=endpoint_id,\n",
        "            project=project_id,\n",
        "            location=region,\n",
        "            credentials=credentials,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_gcs_client(\n",
        "        cls, credentials: \"Credentials\", project_id: str\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a GCS client.\n",
        "\n",
        "        Returns:\n",
        "            A configured GCS client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import storage\n",
        "\n",
        "        return storage.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_index_client(\n",
        "        cls, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a Matching Engine Index client.\n",
        "\n",
        "        Returns:\n",
        "            A configured Matching Engine Index client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        # PARENT = f\"projects/{project_id}/locations/{region}\"\n",
        "        ENDPOINT = f\"{region}-aiplatform.googleapis.com\"\n",
        "        return aiplatform_v1.IndexServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT), credentials=credentials\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_index_endpoint_client(\n",
        "        cls, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a Matching Engine Index Endpoint client.\n",
        "\n",
        "        Returns:\n",
        "            A configured Matching Engine Index Endpoint client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        # PARENT = f\"projects/{project_id}/locations/{region}\"\n",
        "        ENDPOINT = f\"{region}-aiplatform.googleapis.com\"\n",
        "        return aiplatform_v1.IndexEndpointServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT), credentials=credentials\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _init_aiplatform(\n",
        "        cls,\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        gcs_bucket_name: str,\n",
        "        credentials: \"Credentials\",\n",
        "    ) -> None:\n",
        "        \"\"\"Configures the aiplatform library.\n",
        "\n",
        "        Args:\n",
        "            project_id: The GCP project id.\n",
        "            region: The default location making the API calls. It must have\n",
        "            the same location as the GCS bucket and must be regional.\n",
        "            gcs_bucket_name: GCS staging location.\n",
        "            credentials: The GCS Credentials object.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform\n",
        "\n",
        "        logger.debug(\n",
        "            f\"Initializing AI Platform for project {project_id} on \"\n",
        "            f\"{region} and for {gcs_bucket_name}.\"\n",
        "        )\n",
        "        aiplatform.init(\n",
        "            project=project_id,\n",
        "            location=region,\n",
        "            staging_bucket=gcs_bucket_name,\n",
        "            credentials=credentials,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_default_embeddings(cls) -> TensorflowHubEmbeddings:\n",
        "        \"\"\"This function returns the default embedding.\"\"\"\n",
        "        return TensorflowHubEmbeddings()\n",
        "\n",
        "# Utility functions to create Index and deploy the index to an Endpoint\n",
        "from datetime import datetime\n",
        "import time\n",
        "import logging\n",
        "\n",
        "from google.cloud import aiplatform_v1 as aipv1\n",
        "from google.protobuf import struct_pb2\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "class MatchingEngineUtils:\n",
        "    def __init__(self, project_id: str, region: str, index_name: str):\n",
        "        self.project_id = project_id\n",
        "        self.region = region\n",
        "        self.index_name = index_name\n",
        "        self.index_endpoint_name = f\"{self.index_name}-endpoint\"\n",
        "        self.PARENT = f\"projects/{self.project_id}/locations/{self.region}\"\n",
        "\n",
        "        ENDPOINT = f\"{self.region}-aiplatform.googleapis.com\"\n",
        "        # set index client\n",
        "        self.index_client = aipv1.IndexServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT)\n",
        "        )\n",
        "        # set index endpoint client\n",
        "        self.index_endpoint_client = aipv1.IndexEndpointServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT)\n",
        "        )\n",
        "\n",
        "    def get_index(self):\n",
        "        # Check if index exists\n",
        "        request = aipv1.ListIndexesRequest(parent=self.PARENT)\n",
        "        page_result = self.index_client.list_indexes(request=request)\n",
        "        indexes = [\n",
        "            response.name\n",
        "            for response in page_result\n",
        "            if response.display_name == self.index_name\n",
        "        ]\n",
        "\n",
        "        if len(indexes) == 0:\n",
        "            return None\n",
        "        else:\n",
        "            index_id = indexes[0]\n",
        "            request = aipv1.GetIndexRequest(name=index_id)\n",
        "            index = self.index_client.get_index(request=request)\n",
        "            return index\n",
        "\n",
        "    def get_index_endpoint(self):\n",
        "        # Check if index endpoint exists\n",
        "        request = aipv1.ListIndexEndpointsRequest(parent=self.PARENT)\n",
        "        page_result = self.index_endpoint_client.list_index_endpoints(request=request)\n",
        "        index_endpoints = [\n",
        "            response.name\n",
        "            for response in page_result\n",
        "            if response.display_name == self.index_endpoint_name\n",
        "        ]\n",
        "\n",
        "        if len(index_endpoints) == 0:\n",
        "            return None\n",
        "        else:\n",
        "            index_endpoint_id = index_endpoints[0]\n",
        "            request = aipv1.GetIndexEndpointRequest(name=index_endpoint_id)\n",
        "            index_endpoint = self.index_endpoint_client.get_index_endpoint(\n",
        "                request=request\n",
        "            )\n",
        "            return index_endpoint\n",
        "\n",
        "    def create_index(\n",
        "        self,\n",
        "        embedding_gcs_uri: str,\n",
        "        dimensions: int,\n",
        "        index_update_method: str = \"streaming\",\n",
        "        index_algorithm: str = \"tree-ah\",\n",
        "    ):\n",
        "        # Get index\n",
        "        index = self.get_index()\n",
        "        # Create index if does not exists\n",
        "        if index:\n",
        "            logger.info(f\"Index {self.index_name} already exists with id {index.name}\")\n",
        "        else:\n",
        "            logger.info(f\"Index {self.index_name} does not exists. Creating index ...\")\n",
        "\n",
        "            if index_update_method == \"streaming\":\n",
        "                index_update_method = aipv1.Index.IndexUpdateMethod.STREAM_UPDATE\n",
        "            else:\n",
        "                index_update_method = aipv1.Index.IndexUpdateMethod.BATCH_UPDATE\n",
        "\n",
        "            treeAhConfig = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=500),\n",
        "                    \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
        "                }\n",
        "            )\n",
        "            if index_algorithm == \"treeah\":\n",
        "                algorithmConfig = struct_pb2.Struct(\n",
        "                    fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
        "                )\n",
        "            else:\n",
        "                algorithmConfig = struct_pb2.Struct(\n",
        "                    fields={\n",
        "                        \"bruteForceConfig\": struct_pb2.Value(\n",
        "                            struct_value=struct_pb2.Struct()\n",
        "                        )\n",
        "                    }\n",
        "                )\n",
        "            config = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"dimensions\": struct_pb2.Value(number_value=dimensions),\n",
        "                    \"approximateNeighborsCount\": struct_pb2.Value(number_value=150),\n",
        "                    \"distanceMeasureType\": struct_pb2.Value(\n",
        "                        string_value=\"DOT_PRODUCT_DISTANCE\"\n",
        "                    ),\n",
        "                    \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
        "                    \"shardSize\": struct_pb2.Value(string_value=\"SHARD_SIZE_SMALL\"),\n",
        "                }\n",
        "            )\n",
        "            metadata = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"config\": struct_pb2.Value(struct_value=config),\n",
        "                    \"contentsDeltaUri\": struct_pb2.Value(\n",
        "                        string_value=embedding_gcs_uri\n",
        "                    ),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            index_request = {\n",
        "                \"display_name\": self.index_name,\n",
        "                \"description\": \"Index for LangChain demo\",\n",
        "                \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
        "                \"index_update_method\": index_update_method,\n",
        "            }\n",
        "\n",
        "            r = self.index_client.create_index(parent=self.PARENT, index=index_request)\n",
        "            logger.info(\n",
        "                f\"Creating index with long running operation {r._operation.name}\"\n",
        "            )\n",
        "\n",
        "            # Poll the operation until it's done successfullly.\n",
        "            logging.info(\"Poll the operation to create index ...\")\n",
        "            while True:\n",
        "                if r.done():\n",
        "                    break\n",
        "                time.sleep(60)\n",
        "                print(\".\", end=\"\")\n",
        "\n",
        "            index = r.result()\n",
        "            logger.info(\n",
        "                f\"Index {self.index_name} created with resource name as {index.name}\"\n",
        "            )\n",
        "\n",
        "        return index\n",
        "\n",
        "    def deploy_index(\n",
        "        self,\n",
        "        machine_type: str = \"e2-standard-2\",\n",
        "        min_replica_count: int = 2,\n",
        "        max_replica_count: int = 10,\n",
        "        network: str = None,\n",
        "    ):\n",
        "        try:\n",
        "            # Get index if exists\n",
        "            index = self.get_index()\n",
        "            if not index:\n",
        "                raise Exception(\n",
        "                    f\"Index {self.index_name} does not exists. Please create index before deploying.\"\n",
        "                )\n",
        "\n",
        "            # Get index endpoint if exists\n",
        "            index_endpoint = self.get_index_endpoint()\n",
        "            # Create Index Endpoint if does not exists\n",
        "            if index_endpoint:\n",
        "                logger.info(\n",
        "                    f\"Index endpoint {self.index_endpoint_name} already exists with resource \"\n",
        "                    + f\"name as {index_endpoint.name} and endpoint domain name as \"\n",
        "                    + f\"{index_endpoint.public_endpoint_domain_name}\"\n",
        "                )\n",
        "            else:\n",
        "                logger.info(\n",
        "                    f\"Index endpoint {self.index_endpoint_name} does not exists. Creating index endpoint...\"\n",
        "                )\n",
        "                index_endpoint_request = {\"display_name\": self.index_endpoint_name}\n",
        "\n",
        "                if network:\n",
        "                    index_endpoint_request[\"network\"] = network\n",
        "                else:\n",
        "                    index_endpoint_request[\"public_endpoint_enabled\"] = True\n",
        "\n",
        "                r = self.index_endpoint_client.create_index_endpoint(\n",
        "                    parent=self.PARENT, index_endpoint=index_endpoint_request\n",
        "                )\n",
        "                logger.info(\n",
        "                    f\"Deploying index to endpoint with long running operation {r._operation.name}\"\n",
        "                )\n",
        "\n",
        "                logger.info(\"Poll the operation to create index endpoint ...\")\n",
        "                while True:\n",
        "                    if r.done():\n",
        "                        break\n",
        "                    time.sleep(60)\n",
        "                    print(\".\", end=\"\")\n",
        "\n",
        "                index_endpoint = r.result()\n",
        "                logger.info(\n",
        "                    f\"Index endpoint {self.index_endpoint_name} created with resource \"\n",
        "                    + f\"name as {index_endpoint.name} and endpoint domain name as \"\n",
        "                    + f\"{index_endpoint.public_endpoint_domain_name}\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create index endpoint {self.index_endpoint_name}\")\n",
        "            raise e\n",
        "\n",
        "        # Deploy Index to endpoint\n",
        "        try:\n",
        "            # Check if index is already deployed to the endpoint\n",
        "            for d_index in index_endpoint.deployed_indexes:\n",
        "                if d_index.index == index.name:\n",
        "                    logger.info(\n",
        "                        f\"Skipping deploying Index. Index {self.index_name}\"\n",
        "                        + f\"already deployed with id {index.name} to the index endpoint {self.index_endpoint_name}\"\n",
        "                    )\n",
        "                    return index_endpoint\n",
        "\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "            deployed_index_id = f\"{self.index_name.replace('-', '_')}_{timestamp}\"\n",
        "            deploy_index = {\n",
        "                \"id\": deployed_index_id,\n",
        "                \"display_name\": deployed_index_id,\n",
        "                \"index\": index.name,\n",
        "                \"dedicated_resources\": {\n",
        "                    \"machine_spec\": {\n",
        "                        \"machine_type\": machine_type,\n",
        "                    },\n",
        "                    \"min_replica_count\": min_replica_count,\n",
        "                    \"max_replica_count\": max_replica_count,\n",
        "                },\n",
        "            }\n",
        "            logger.info(f\"Deploying index with request = {deploy_index}\")\n",
        "            r = self.index_endpoint_client.deploy_index(\n",
        "                index_endpoint=index_endpoint.name, deployed_index=deploy_index\n",
        "            )\n",
        "\n",
        "            # Poll the operation until it's done successfullly.\n",
        "            logger.info(\"Poll the operation to deploy index ...\")\n",
        "            while True:\n",
        "                if r.done():\n",
        "                    break\n",
        "                time.sleep(60)\n",
        "                print(\".\", end=\"\")\n",
        "\n",
        "            logger.info(\n",
        "                f\"Deployed index {self.index_name} to endpoint {self.index_endpoint_name}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(\n",
        "                f\"Failed to deploy index {self.index_name} to the index endpoint {self.index_endpoint_name}\"\n",
        "            )\n",
        "            raise e\n",
        "\n",
        "        return index_endpoint\n",
        "\n",
        "    def get_index_and_endpoint(self):\n",
        "        # Get index id if exists\n",
        "        index = self.get_index()\n",
        "        index_id = index.name if index else \"\"\n",
        "\n",
        "        # Get index endpoint id if exists\n",
        "        index_endpoint = self.get_index_endpoint()\n",
        "        index_endpoint_id = index_endpoint.name if index_endpoint else \"\"\n",
        "\n",
        "        return index_id, index_endpoint_id\n",
        "\n",
        "    def delete_index(self):\n",
        "        # Check if index exists\n",
        "        index = self.get_index()\n",
        "\n",
        "        # create index if does not exists\n",
        "        if index:\n",
        "            # Delete index\n",
        "            index_id = index.name\n",
        "            logger.info(f\"Deleting Index {self.index_name} with id {index_id}\")\n",
        "            self.index_client.delete_index(name=index_id)\n",
        "        else:\n",
        "            raise Exception(\"Index {index_name} does not exists.\")\n",
        "\n",
        "    def delete_index_endpoint(self):\n",
        "        # Check if index endpoint exists\n",
        "        index_endpoint = self.get_index_endpoint()\n",
        "\n",
        "        # Create Index Endpoint if does not exists\n",
        "        if index_endpoint:\n",
        "            logger.info(\n",
        "                f\"Index endpoint {self.index_endpoint_name}  exists with resource \"\n",
        "                + f\"name as {index_endpoint.name} and endpoint domain name as \"\n",
        "                + f\"{index_endpoint.public_endpoint_domain_name}\"\n",
        "            )\n",
        "\n",
        "            index_endpoint_id = index_endpoint.name\n",
        "            index_endpoint = self.index_endpoint_client.get_index_endpoint(\n",
        "                name=index_endpoint_id\n",
        "            )\n",
        "            # Undeploy existing indexes\n",
        "            for d_index in index_endpoint.deployed_indexes:\n",
        "                logger.info(\n",
        "                    f\"Undeploying index with id {d_index.id} from Index endpoint {self.index_endpoint_name}\"\n",
        "                )\n",
        "                request = aipv1.UndeployIndexRequest(\n",
        "                    index_endpoint=index_endpoint_id, deployed_index_id=d_index.id\n",
        "                )\n",
        "                r = self.index_endpoint_client.undeploy_index(request=request)\n",
        "                response = r.result()\n",
        "                logger.info(response)\n",
        "\n",
        "            # Delete index endpoint\n",
        "            logger.info(\n",
        "                f\"Deleting Index endpoint {self.index_endpoint_name} with id {index_endpoint_id}\"\n",
        "            )\n",
        "            self.index_endpoint_client.delete_index_endpoint(name=index_endpoint_id)\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"Index endpoint {self.index_endpoint_name} does not exists.\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Initialize embedding directory with a null vector*"
      ],
      "metadata": {
        "id": "lkE0QpAEIo7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9u0Bu56kWWm",
        "outputId": "23dc02d8-bae3-40d0-cffa-e0fb1af07117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ gsutil cp embeddings_0.json gs://argolis-project-340214-me-bucket/init_index/embeddings_0.json\n",
            "Copying file://embeddings_0.json [Content-Type=application/json]...\n",
            "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
            "Operation completed over 1 objects/3.8 KiB.                                      \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# dummy embedding\n",
        "init_embedding = {\"id\": str(uuid.uuid4()), \"embedding\": list(np.zeros(ME_DIMENSIONS))}\n",
        "\n",
        "# dump embedding to a local file\n",
        "with open(\"embeddings_0.json\", \"w\") as f:\n",
        "    json.dump(init_embedding, f)\n",
        "\n",
        "# write embedding to Cloud Storage\n",
        "! set -x && gsutil cp embeddings_0.json gs://{ME_EMBEDDING_DIR}/init_index/embeddings_0.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Create a Matching Engine Index and deploy to a endpoint*"
      ],
      "metadata": {
        "id": "soiuCisTJp9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1rxq9YnWrw5",
        "outputId": "6dbccb8a-5ecd-4dda-f9bf-85dd80626c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "projects/742458474659/locations/us-central1/indexes/6528535007873466368\n",
            "Index endpoint resource name: projects/742458474659/locations/us-central1/indexEndpoints/5965585054452154368\n",
            "Index endpoint public domain name: 1784211276.us-central1-742458474659.vdb.vertexai.goog\n",
            "Deployed indexes on the index endpoint:\n",
            "    argolis_project_340214_me_index_20231014181057\n"
          ]
        }
      ],
      "source": [
        "mengine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)\n",
        "\n",
        "index = mengine.create_index(\n",
        "    embedding_gcs_uri=f\"gs://{ME_EMBEDDING_DIR}/init_index\",\n",
        "    dimensions=ME_DIMENSIONS,\n",
        "    index_update_method=\"streaming\",\n",
        "    index_algorithm=\"tree-ah\",\n",
        ")\n",
        "if index:\n",
        "    print(index.name)\n",
        "\n",
        "index_endpoint = mengine.deploy_index()\n",
        "if index_endpoint:\n",
        "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
        "    print(\n",
        "        f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\"\n",
        "    )\n",
        "    print(\"Deployed indexes on the index endpoint:\")\n",
        "    for d in index_endpoint.deployed_indexes:\n",
        "        print(f\"    {d.id}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
        "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
        "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")\n",
        "print(f\"ME_INDEX_NAME={ME_INDEX_NAME}\")\n",
        "print(f\"ME_BUCKET={ME_BUCKET}\")\n",
        "print(f\"PROJECT_ID={PROJECT_ID}\")\n",
        "print(f\"ME_REGION={ME_REGION}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6RmKdSNwmLS",
        "outputId": "def9cd24-a358-4b7a-8f06-564c148d6d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ME_INDEX_ID=projects/742458474659/locations/us-central1/indexes/6528535007873466368\n",
            "ME_INDEX_ENDPOINT_ID=projects/742458474659/locations/us-central1/indexEndpoints/5965585054452154368\n",
            "ME_INDEX_NAME=argolis-project-340214-me-index\n",
            "ME_BUCKET=matching_engine_bucket\n",
            "PROJECT_ID=argolis-project-340214\n",
            "ME_REGION=us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Create Embedding Engine & Build a Data Store*"
      ],
      "metadata": {
        "id": "JKWQSeChKYUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Text Embeddings model\n",
        "embedding = VertexAIEmbeddings()\n"
      ],
      "metadata": {
        "id": "cXNxt4cwyMes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Matching Engine as Vector Store\n",
        "me = MatchingEngine.from_components(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=ME_REGION,\n",
        "    gcs_bucket_name=f'gs://{ME_BUCKET}',\n",
        "    embedding=embedding,\n",
        "    index_id=ME_INDEX_ID,\n",
        "    endpoint_id=ME_INDEX_ENDPOINT_ID)\n"
      ],
      "metadata": {
        "id": "UWSxHHO4YNZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YozENqoAmm9"
      },
      "source": [
        "## Absorb documents, split them into chunks and capture metadata\n",
        "\n",
        "*Loading...*\n",
        "This takes some time: 1min++"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ME_BUCKET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yqLac6INgt0",
        "outputId": "d56f55ef-f3fd-4aa4-fdf2-0145e5eea112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matching_engine_bucket\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td4rD2MQtM1O",
        "outputId": "0bda89e4-82eb-441c-bad7-a4a800294b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "loader = GCSDirectoryLoader(project_name=PROJECT_ID, bucket=\"contractunderstandingatticusdataset\")\n",
        "contractdocs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzLAH66JfqwZ"
      },
      "outputs": [],
      "source": [
        "print(f\"# of documents = {len(contractdocs)}\")\n",
        "print(contractdocs[0].metadata)\n",
        "print( contractdocs[0].json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Da7_1bpFGpb"
      },
      "source": [
        "*Split documents into chunks as needed by the token limit of the LLM and let there be an overlap between the chunks*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E_qzSuMFHKt",
        "outputId": "5bd21b88-7373-4e4d-c101-8c22b703a776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of document splits = 2150\n"
          ]
        }
      ],
      "source": [
        "# split the documents into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "doc_splits = text_splitter.split_documents(contractdocs)\n",
        "print(f\"# of document splits = {len(doc_splits)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Capture Metadata*"
      ],
      "metadata": {
        "id": "lUG4baXr8QhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add chunk number to metadata\n",
        "for idx, split in enumerate(doc_splits):\n",
        "    #print(idx)\n",
        "    split.metadata[\"chunk\"] = idx\n",
        "\n",
        "# Separate doc_splits to semantic data and meta data\n",
        "texts = [doc.page_content for doc in doc_splits]\n",
        "metadatas = [\n",
        "    [\n",
        "        {\"namespace\": \"source\", \"allow_list\": [doc.metadata[\"source\"]]},\n",
        "        {\"namespace\": \"document_name\", \"allow_list\": [doc.metadata[\"source\"].split(\"/\")[-1]]},\n",
        "        {\"namespace\": \"chunk\", \"allow_list\": [str(doc.metadata[\"chunk\"])]},\n",
        "    ]\n",
        "    for doc in doc_splits\n",
        "]\n",
        "print(f\"# of document splits = {len(doc_splits)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxwvbX3227xm",
        "outputId": "fb063d8a-96eb-4581-c666-62dc8ad84307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of document splits = 2150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadatas[3]"
      ],
      "metadata": {
        "id": "64kH7M1XF6Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load semantic data as texts and metadata into Matching Engine\n",
        "This takes time: 7mins ++"
      ],
      "metadata": {
        "id": "8I7s_89e8xI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids = me.add_texts(texts=texts, metadatas=metadatas)"
      ],
      "metadata": {
        "id": "jjMsKR6t2lU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Do some testing of Matching Engine*"
      ],
      "metadata": {
        "id": "6-RAqBDlHHrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test whether search from vector store is working\n",
        "me.similarity_search(\"image\", k=2)"
      ],
      "metadata": {
        "id": "dHqWi45k1cZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "me.similarity_search(\"Twin Cities Power Holdings\", k=2, search_distance=0.4)\n"
      ],
      "metadata": {
        "id": "yj8sWZFq9smL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCftNbU-wp0"
      },
      "source": [
        "## Obtain handle to the retriever\n",
        "\n",
        "We will use the native retriever provided by Chroma DB to perform similarity search within the contracts document vector store among the different document chunks so as to return that document chunk which has the lowest vectoral \"distance\" with the incoming user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb2VCn6e-0zp"
      },
      "outputs": [],
      "source": [
        "# Retriever configuration\n",
        "NUMBER_OF_RESULTS = 10\n",
        "SEARCH_DISTANCE_THRESHOLD = 0.6\n",
        "\n",
        "# Expose index to the retriever\n",
        "retriever = me.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": NUMBER_OF_RESULTS,\n",
        "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcwQNvfN_6Rn"
      },
      "source": [
        "## Define a Retrieval QA Chain to use retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElWUO3fQAMaH"
      },
      "outputs": [],
      "source": [
        "llm = VertexAI(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "# We use Vertex PaLM Text API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHYlgYQhFTQW"
      },
      "source": [
        "## Leverage LLM to search from retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUZDkQOrGb9X"
      },
      "source": [
        "*Example:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGTS8x7TFoOn",
        "outputId": "4ad42771-8b42-407e-90be-87c503c80715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Who all entered into agreement with Sagebrush?', 'result': ' The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', 'source_documents': [Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.70986008644104}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.709692120552063}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.709692120552063}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.709692120552063}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.709692120552063}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899971961975098}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899971961975098}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899971961975098}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899971961975098}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899837851524353})]}\n"
          ]
        }
      ],
      "source": [
        "query = \"Who all entered into agreement with Sagebrush?\"\n",
        "result = qa({\"query\": query})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgYa4k1WiolM"
      },
      "source": [
        "## Build a Front End"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRjBQsXqirbC"
      },
      "outputs": [],
      "source": [
        "def chatbot(inputtext):\n",
        "    result = qa({\"query\": inputtext})\n",
        "\n",
        "    return result['result'], get_public_url(result['source_documents'][0].metadata['source']), result['source_documents'][0].metadata['source']\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "def get_public_url(uri):\n",
        "    \"\"\"Returns the public URL for a file in Google Cloud Storage.\"\"\"\n",
        "    # Split the URI into its components\n",
        "    components = uri.split(\"/\")\n",
        "\n",
        "    # Get the bucket name\n",
        "    bucket_name = components[2]\n",
        "\n",
        "    # Get the file name\n",
        "    file_name = components[3]\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "    return blob.public_url\n",
        "\n",
        "\n",
        "print(\"Launching Gradio\")\n",
        "\n",
        "iface = gr.Interface(fn=chatbot,\n",
        "                     inputs=[gr.Textbox(label=\"Query\")],\n",
        "                     examples=[\"What is the agreement made by Twin Cities Power Holdings\", \"What is the agreement between MICOA & Stratton Cheeseman\", \"What is the commission % that Stratton Cheeseman will get from MICOA and how much will they get if MICOA's revenues are $100\"],\n",
        "                     title=\"Contract Analyst\",\n",
        "                     outputs=[gr.Textbox(label=\"Response\"),\n",
        "                              gr.Textbox(label=\"URL\"),\n",
        "                              gr.Textbox(label=\"Cloud Storage URI\")],\n",
        "                     theme=gr.themes.Soft)\n",
        "\n",
        "iface.launch(share=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
