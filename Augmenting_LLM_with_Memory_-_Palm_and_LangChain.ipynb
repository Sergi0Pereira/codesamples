{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Augmenting LLM with Memory  -  Palm2 & LangChain"
      ],
      "metadata": {
        "id": "Uj7R48-ZXmTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/guruvittal/codesamples/blob/main/Augmenting_LLM_with_Memory_-_Palm_and_LangChain.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/guruvittal/codesamples/blob/main/Augmenting_LLM_with_Memory_-_Palm_and_LangChain.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/guruvittal/codesamples/main/Augmenting_LLM_with_Memory_-_Palm_and_LangChain.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "LrNDf_WNl-2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation & Authentication"
      ],
      "metadata": {
        "id": "SPG8eRQCcNQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install google-cloud-aiplatform & langchain"
      ],
      "metadata": {
        "id": "JDBA8aH3h7kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain and related libraries\n",
        "!pip install --upgrade langchain unstructured\n",
        "\n",
        "# Install Vertex AI LLM SDK\n",
        "! pip install --upgrade google-cloud-aiplatform\n",
        "\n",
        "# Install Gradio for a simple GUI\n",
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "id": "IdsE3DEJcM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Authenticate"
      ],
      "metadata": {
        "id": "ptwMB9pqcniz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "\n"
      ],
      "metadata": {
        "id": "HP80SWi0rIBL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Vertex & LangChain Libraries"
      ],
      "metadata": {
        "id": "4lcArn48r6pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*LangChain & Vertex Standard Libraries*"
      ],
      "metadata": {
        "id": "kuT3CSItiJjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use LangChain\n",
        "import langchain\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Use COnversation Buffer Memory & Conversation Chain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "# Use standard libraries\n",
        "from pydantic import BaseModel, root_validator\n",
        "from typing import Any, Mapping, Optional, List, Dict\n",
        "\n",
        "# Simple Front end libraries\n",
        "import gradio as gr\n",
        "import markdown\n",
        "\n",
        "import vertexai\n",
        "\n",
        "from langchain.llms import VertexAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "906zdZGA6wp_",
        "outputId": "d4b3219c-405e-4434-c988-7a30a52c2596"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain version: 0.0.316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Vertex AI"
      ],
      "metadata": {
        "id": "7yHf4ipxYyfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Initialize Project & overall platform*"
      ],
      "metadata": {
        "id": "82ItKpTFiVdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"PROJECT_ID\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n"
      ],
      "metadata": {
        "id": "EZe8iS2CY2E8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017e1001-b543-41ce-948e-72d6528aebcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = VertexAI(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "b97yXMkW06gy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Store Chat History into Memory Buffer and pass it to LLM"
      ],
      "metadata": {
        "id": "mDe7g9ImmBn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=memory)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kYS24OYutEjK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoke the LLM to test the memory\n"
      ],
      "metadata": {
        "id": "7YozENqoAmm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi there! I am Sam\")"
      ],
      "metadata": {
        "id": "1XgbOt1stP5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I live in Dublin, Ohio\")"
      ],
      "metadata": {
        "id": "VKx0LFjD2EnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Which is the nearest airport for me to visit\")"
      ],
      "metadata": {
        "id": "9ORpGjCL2geT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Front End"
      ],
      "metadata": {
        "id": "FgYa4k1WiolM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newmemory = ConversationBufferMemory()\n",
        "newconversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=newmemory)\n",
        "\n",
        "def chatbot(inputtext):\n",
        "    return newconversation.predict(input=inputtext)\n",
        "\n",
        "print(\"Launching Gradio\")\n",
        "\n",
        "iface = gr.Interface(fn=chatbot,\n",
        "                     inputs=[gr.Textbox(label=\"Query\")],\n",
        "                     examples=[\"I live in Plano, Texas\", \"My flight is at 10PM\", \"Which is the nearest airport and when shd I start from here to get my flight\"],\n",
        "                     title=\"Chat With a Bison\",\n",
        "                     outputs=[gr.Textbox(label=\"Response\")])\n",
        "\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "CRjBQsXqirbC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
